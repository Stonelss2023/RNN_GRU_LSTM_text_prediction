{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9816b6-ccb5-4676-a15d-a8f372dc7d53",
   "metadata": {},
   "source": [
    "# RNN/GRU/LSTM模型在文本预测生成上的性能差异\n",
    "主要内容：\n",
    "- 数据预处理\n",
    "    - 下载与加载 (Jaychou lyrics dataset)\n",
    "    - 清洗与标准化\n",
    "    - 采样\n",
    "    - 数据集呈现\n",
    "- RNN模型结构\n",
    "    - \n",
    "- GRU\n",
    "- LSTM\n",
    "- 训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bb0bc-54f9-43fb-a8c2-5aadfac3bbdf",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeca7dc-3b05-4edb-acdd-a8305c26abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the lexicon\n",
    "def preprocess(corpus_chars, num_chars=10000):\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[:num_chars]\n",
    "\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = {char: i for i, char in enumerate(idx_to_char)}\n",
    "    vocab_size = len(cahr_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "    return corpus_chars, idx_to_char, char_to_idx, vocab_size, corpus_indices\n",
    "\n",
    "# Sampling\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' is torch.cuda.is_available() else 'cpu')\n",
    "    corpus_indices = trorch.tensor(corpus_indices, dtype=torch.float32, device=device)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i+1, i + num_stpes + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "030b0713-a6ad-4f1e-86e6-9003dfa71c01",
   "metadata": {},
   "source": [
    "【并行序列处理的设计优势】\n",
    "1. 允许并行处理多个序列 → 提高效率\n",
    "2. 保持原始序列中词的顺序，有利于捕捉长距离依赖 (和随机采样相比)\n",
    "\n",
    "【妙手】\n",
    "1. 原始长序列(完整字符级语料库)被分成batch_size个等长并行序列,每个并行序列长度时batch_len\n",
    "2. 每次迭代从每个并行序列中选取num_steps长度的数据\n",
    "如此，每个批次的样本数始终保持为 batch_size\n",
    "batch_size = 并行序列数 = 单batch样本数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1fabf-45be-4679-ad2d-c1c8a61b0f04",
   "metadata": {},
   "source": [
    "# 2. Dataset Synthesized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71301343-bdc1-462b-aaf0-1e4ac18b7044",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2412337089.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 26\u001b[1;36m\u001b[0m\n\u001b[1;33m    def get_random_iter(self, corpus_chars, self.num_chars):\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class JaychouDataset:\n",
    "    def __init__(self, num_chars=10000):\n",
    "        self.num_chars = num_chars\n",
    "        self.corpus_chars = None\n",
    "        self.idx_to_char = None\n",
    "        self.char_to_idx = None\n",
    "        self.vocab_size = None\n",
    "        self.corpus_indices = None\n",
    "\n",
    "    def load_data(self):\n",
    "        if self.corpus_chars is None:\n",
    "            print(\"Loading data...\")\n",
    "            self.corpus_chars = download_data()\n",
    "            (self.corpus_chars, \n",
    "             self.idx_to_char,\n",
    "             self.vocab_size,\n",
    "             self.vocab_size, \n",
    "             self.corpus_indices\n",
    "            ) = preprocess_data(self.corpus_chars, self.num_chars)\n",
    "            print(\"Data loaded and processed.\")\n",
    "\n",
    "    def get_random_iter(self, batch_size, num_steps):\n",
    "        self.load_data()\n",
    "        return data_iter_random(self.corpus_indices, batch_size, num_steps)\n",
    "\n",
    "    def get_consecutive_iter(self, batch_size, num_steps):\n",
    "        self.load_data()\n",
    "        return data_iter_consecutive(self.corpus_indices, batch_size, num_steps)\n",
    "\n",
    "    def get_corpus_chars(self):\n",
    "        self.load_data()\n",
    "        return self.corpus_chars\n",
    "\n",
    "    def get_vocab_info(self):\n",
    "        self.load_data()\n",
    "        return self.idx_to_char, self.char_to_idx, self.vocab_size\n",
    "\n",
    "    def get_corpus_indices(self):\n",
    "        self.load_data()\n",
    "        return self.corpus_indices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29ffbbeb-eeb2-4e12-ab58-ee8f3d65de77",
   "metadata": {},
   "source": [
    "封装完整的数据处理流程，统一管理数据集相关信息. 所谓\"接口\",就是类/模块对外提供的调用方法\n",
    "1. 它们封装了内部复杂的数据处理逻辑，但对外提供简单统一的调用方式。这个项目写成封装形式的根本原因在于：sampling脚本定义了两种采样方式。\n",
    "2. 封装成类的第二个优势：延迟加载(Lazy Loading) → 在真正需要使用数据时才进行加载器，而不是在创建对象时就立即加载所有数据。在__init__过程中只设置参数，不加载数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a3286-0025-44d5-b60a-7fe78627170f",
   "metadata": {},
   "source": [
    "1. hidden_size != hidden_layer → hidden_size是每个时间步隐藏状态的向量维度大小，也等价于抽象意义上的神经元数量。 其数量大小所代表的具体含义不是预定义的、不是可解释的→学出来的抽象特征\n",
    "较大的hidden_size可以捕捉更复杂的模式，但会增加过拟合风险\n",
    "2. F.one_hot期望参数→输入索引+词汇表大小\n",
    "输入索引是DL6_3_2jzlyrics_processor中的corpus_indices\n",
    ".long()把输入转换为长整型，因为one_hot函数期望整数索引\n",
    ".float()将整数类型转换为浮点类型→神经网络通常使用浮点数计算→支持梯度计算和反向传播\n",
    "3. Y.shape[-1]表示Y张量的最后一维大小（对应隐藏状态大小）\n",
    "重塑后：dim1→时间步*批次 dim2→hidden_size  →  方便应用全连接层\n",
    "output形状 → [sequence_length * batch_size, vocab_size] → 包含所有时间步预测结果的\n",
    "4. dense操作每个时间步输出的隐藏状态→接受隐藏状态的输出向量维度，映射到vocab上，预测下一步的词汇概率分布\n",
    "5. rnn_layer.bidirectional 处理双向RNN的hidden_size问题→判断是否要乘2\n",
    "6. return output, self.state → output 是模型主要输出，针对每一步的字符概率分布预测[batch_size*sequence_length, vocab_size]\n",
    "   self.state 对于RNN→最终隐藏状态\n",
    "              对于LSTM→(hidden_state, cell_state)\n",
    "              对于GRU→最后一个时间步隐藏状态\n",
    "   [num_layers*num_directions, batch_size, hiddden_size]\n",
    "\n",
    "7. tensor_size的关键点解释\n",
    "(1) inputs→[batch_size, sequence_length] 例如[64, 100]表示64个样本，样本时间步为100\n",
    "(2) one_hot编码以后→[batch_size, sequence_length, vocab_size] → [64, 100, 5000]\n",
    "(3) RNN层 Y→[batch_size, sequence_length, hidden_size] → [64, 100, 256](信息抽象提炼)\n",
    "          state:\n",
    "           RNN/GRU → [num_layers*num_directions, batch_size, hidden_size]\n",
    "           LSTM → [hidden_state, cell_state] → [2, 64, 256] （假设有2层单向RNN）\n",
    "8. 在forward方法中又传入一个state→it can be self.state defined in the class RNNModel, it can also be a brand new state conveyed by the user. This flexibility entitles the user with the right to choose the initial state when resetting.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2aad7-8583-4046-9001-4a7ce4be4bbc",
   "metadata": {},
   "source": [
    "# 3. RNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8696b01-86ee-45c1-b9a1-b16ba02ead8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Linear(self.hidden_size, vocab_size)\n",
    "        self.state = None #初始隐藏状态为空\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.long(), self.vocab_size).float()\n",
    "        Y, self.state = self.rnn(X, state)\n",
    "        output = self.dense (Y.reshape(-1, Y.shape[-1]))\n",
    "        return output, self.state\n",
    "\n",
    "    def predict(self, prefix, num_chars, device, idx_to_char, char_to_idx, temperature=0.3):\n",
    "        state = None\n",
    "        output = [char_to_idx[prefix[0]]] #使用给定的前缀(prefix)初始化生成过程\n",
    "\n",
    "        for t in range(num_chars + len(prefix) - 1):\n",
    "              X = torch.tensor([output[-1]], device=device).view(-1, 1)\n",
    "              if state is not None:\n",
    "                  if isinstance(statte, tuple):\n",
    "                    state = state[0].to(device), state[1].to(device)) #LSTM的隐藏状态→(h, c)\n",
    "                  else:\n",
    "                      state = state.to(device)\n",
    "\n",
    "              (Y, state) = self(X, state)\n",
    "\n",
    "              if t < len(prefix) - 1:\n",
    "                  output.append(char_to_idx[prefix[t + 1]])\n",
    "              else:\n",
    "                  scaled_logits = Y / temperature\n",
    "                  probs = F.softmax(scaled_logits, dim=1)\n",
    "                  next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "                  output.append(next_char_idx)\n",
    "\n",
    "        return ''. join([idx_to_char[i] for i in output]) #把字符索引映射回字符,拼接成字符串返回\n",
    "\n",
    "\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grd.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bf1c0-b698-417b-86ca-5ecfa69b6e49",
   "metadata": {},
   "source": [
    "2. X = torch.tensor([output[-1]], device=device).view(-1, 1)\n",
    "准备输入数据 → 获取output列表最后一个元素（最新预测字符的索引）重塑张量为一个列向量\n",
    "3. if isinstance(state, tuple):检查状体是否是元组 → 隐藏状态+单元状态\n",
    "\n",
    "4. softmax运算指定 dim=1 → Y的通常形状是(batch_size, vocab_size),在词汇表维度应用softmax确保所有字符的可能概率和为1\n",
    "5. torch.multinomial(probs, num_samples=1) 从给定的probs概率分布中采样；num_samples=1 → 只采样一次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb53ce4-f5d6-48aa-a0a5-1babc61161fe",
   "metadata": {},
   "source": [
    "# 4. RNN_Train            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1dfd2-99da-4d8c-b043-019db7d5bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from DL_6_3_4jzlyrics_dataset import JaychouDataset\n",
    "from DL6_5_RNN_model import RNNModel, grad_clipping, device\n",
    "\n",
    "def train_and_predict_rnn_pytorch(model, device, corpus_indices, idx_to_char, char_to_idx, \n",
    "                                  num_epochs,num_steps, clipping_theta, batch_size,\n",
    "                                  lr, pred_period, pred_len, prefixes):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(modal.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            l_sum, n, start = 0.0, 0, time.time()\n",
    "            data_iter = sampler.data_iter_consecutive(corpus_indices, batch_size, num_steps, device)\n",
    "            state = None\n",
    "\n",
    "            for X, Y in date_iter:\n",
    "                if state is not None:\n",
    "                    if isinstance(state, tuple):\n",
    "                        state = tuple(s.detach() for s in state) \n",
    "                    else:\n",
    "                        state = state.detach() #清除隐藏状态的梯度信息\n",
    "            (output, state) = model(X, state)\n",
    "            output = output.reshape(-1, len(char_to_idx)) #(batch_size*num_steps, vocab_size)\n",
    "            Y = Y.reshape(-1) #(batch_size*num_steps)\n",
    "\n",
    "            l = loss(output, Y.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(model.parameters(), clipping_theta, device)\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * Y.numel() #损失标量化 乘以批量样本数\n",
    "            n += Y.numel() #更新样本总数\n",
    "\n",
    "        try:\n",
    "            perplexity = math.exp(l_sum / n)\n",
    "        except OverflowError:\n",
    "            perplexity = float('inf') #指数运算溢出则设置perplexity为无穷\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0: \n",
    "            print(f'epoch {epoch + 1}, perplexity {perplexity:.2f}, time {time.time() - start:.2f} sec')\n",
    "            for prefix in prefixes:\n",
    "                print(' -', model.predict(prefix, pred_len, device, # pred_len指定生成文本的长度\n",
    "                                          idx_to_char, char_to_idx)) #使用前缀调用模型的predict方法生成文本\n",
    "            \n",
    "def main():\n",
    "    jz_dataset = JaychouDataset(num_chars=10000)\n",
    "    idx_to_char, char_to_idx, vocab_size = jz_dataset.get_vocab_info()\n",
    "    corpus_indices = jz_dataset.get_corpus.indices()\n",
    "\n",
    "    num_hiddens = 256\n",
    "    rnn_layer = nn.RNN(inputs_size=vocab_size, hidden_size=num_hiddens)\n",
    "    model = RNNModel(rnn_layer, vocab_size).to(device)\n",
    "\n",
    "    num_epochs, batch_size, lr, clipping_theta = 200, 32, 1e-3, 1e-2\n",
    "    pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "    num_steps = 35\n",
    "\n",
    "    train_and_predict_rnn_pytorch(model, device, corpus_indices,\n",
    "                                  idx_to_char, char_to_idx, num_epochs, \n",
    "                                  num_steps, lr, clipping_theta, batch_size, \n",
    "                                  pred_period, pred_len, prefixes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70b7686e-df6d-4496-9de0-a1adc90cbec3",
   "metadata": {},
   "source": [
    "相较于传统的训练流程,多出的部分主要包括：\n",
    "    '隐藏状态梯度信息处理'\n",
    "    'RNN Varients的多个隐藏状态处理'\n",
    "    '依据连续采样和封装的数据维度变化 → loss计算之前'\n",
    "    'optimization前的梯度裁剪步骤'\n",
    "    '评判标准计算:perplexity'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48043baf-5ce0-49ba-bb7b-e1e34134cd41",
   "metadata": {},
   "source": [
    "# 5. GRU_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6024ee-8464-454b-92e4-619f73271b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "num_steps = 35\n",
    "batch_size = 16\n",
    "\n",
    "rnn_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "model = RNNModel(rnn_layer, vocab_size).to(device)\n",
    "\n",
    "num_epochs, lr, clipping_theta = 150, 1e-3, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "\n",
    "train_and_predict_rnn_pytorch(model, device, corpus_indices, \n",
    "                              idx_to_char, char_to_idx, num_epochs, \n",
    "                              num_steps, lr, clipping_theta, batch_size, \n",
    "                              pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be613b5a-d4a6-489e-afa3-f07691f00b2b",
   "metadata": {},
   "source": [
    "# 6. LSTM_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55ae2f-9bf6-4d9f-99b9-524ac7191f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "num_steps = 35\n",
    "batch_size = 16\n",
    "\n",
    "lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "model = RNNModel(lstm_layer, vocab_size).to(device)\n",
    "\n",
    "num_epochs, lr, clipping_theta = 160, 2e-2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "\n",
    "train_and_predict_rnn_pytorch(model, device, corpus_indices, \n",
    "                              idx_to_char, char_to_idx, num_epochs, \n",
    "                              num_steps, lr, clipping_theta, batch_size, \n",
    "                              pred_period, pred_len, prefixes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
